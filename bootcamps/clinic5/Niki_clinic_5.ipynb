{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clinic 5 - Timeseries analysis (Bootcamp)\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "After this clinic/bootcamp you should be able to:\n",
    "\n",
    "- Fetch stock market data using `yfinance`.\n",
    "- Visualize stock trends in different ways.\n",
    "- Compute and interpret moving averages.\n",
    "- Identify seasonality and trends in time series data.\n",
    "- Build predictive models for timeseries (ARIMA).\n",
    "- Identify the right order (p,d,q) for ARIMA models using ACF/PACF plots or cross-validation (AIC criterion)\n",
    "- Make forecasts with timeseries and measure the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Part 1: Mathematical finance \n",
    "\n",
    "Prior to the 1980s, banking and finance were well-known for being \"boring\"; investment banking was distinct from commercial banking and the primary role of the industry was handling \"simple\" (at least in comparison to today) financial instruments, such as loans. Deregulation (in the US happened under the Reagan administration), coupled with an influx of mathematics and computing power have transformed the industry from the \"boring\" business of banking to what it is today. \n",
    "\n",
    "* Advanced mathematics, such as analysis of the [Black-Scholes model](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model), is now essential to finance. \n",
    "* Algorithms are now responsible for making split-second decisions. In fact, [the speed at which light travels is a limitation when designing trading systems](http://www.nature.com/news/physics-in-finance-trading-at-the-speed-of-light-1.16872). \n",
    "* [Machine learning and data mining techniques are popular](http://www.ft.com/cms/s/0/9278d1b6-1e02-11e6-b286-cddde55ca122.html#axzz4G8daZxcl) in the financial sector. For example, **high-frequency trading (HFT)** is a branch of algorithmic trading where computers make thousands of trades in short periods of time, engaging in complex strategies such as statistical arbitrage and market making. HFT was responsible for phenomena such as the [2010 flash crash](https://en.wikipedia.org/wiki/2010_Flash_Crash) and a [2013 flash crash](http://money.cnn.com/2013/04/24/investing/twitter-flash-crash/) prompted by a hacked [Associated Press tweet](http://money.cnn.com/2013/04/23/technology/security/ap-twitter-hacked/index.html?iid=EL) about an attack on the White House.\n",
    "\n",
    "### Installing `yfinance`\n",
    "\n",
    "We will use a package which might not be included in the Anaconda distribution, [**yfinance**](https://pypi.org/project/yfinance/), that can be installed via the command prompt: \n",
    "\n",
    "    pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas_datareader import data as pdr\n",
    "from datetime import datetime\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.style.use('fivethirtyeight')\n",
    "#plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1 Getting and Visualizing Stock Data\n",
    "\n",
    "### The `yfinance` package\n",
    "\n",
    "Here we will use the `yfinance` [package](https://pypi.org/project/yfinance/) to get some data and import them since many of the import functions of `pandas_datareader` have been deprecated.\n",
    "\n",
    "    Ticker: single ticker data\n",
    "    Tickers: multiple tickers' data\n",
    "    download: download market data for multiple tickers\n",
    "    Market: get infomation about a market\n",
    "    Search: quotes and news from search\n",
    "    Sector and Industry: sector and industry information\n",
    "    EquityQuery and Screener: build query to screen market\n",
    "\n",
    "### The `pandas_datareader` package (for historical reasons)\n",
    "\n",
    "`pandas_datareader` extracts data from various internet sources into a pandas DataFrame. Here are the currently listed available [modules](https://pydata.github.io/pandas-datareader/py-modindex.html):\n",
    "\n",
    "\tpandas_datareader.av.forex\t\n",
    "    pandas_datareader.av.quotes\t\n",
    "    pandas_datareader.av.sector\t\n",
    "    pandas_datareader.av.time_series\t\n",
    "    pandas_datareader.bankofcanada\t\n",
    "    pandas_datareader.econdb\t\n",
    "    pandas_datareader.enigma\t\n",
    "    pandas_datareader.eurostat\t\n",
    "    pandas_datareader.famafrench\t\n",
    "    pandas_datareader.fred\t\n",
    "    pandas_datareader.iex.daily\t\n",
    "    pandas_datareader.iex.deep\t\n",
    "    pandas_datareader.iex.market\t\n",
    "    pandas_datareader.iex.ref\t\n",
    "    pandas_datareader.iex.stats\t\n",
    "    pandas_datareader.iex.tops\t\n",
    "    pandas_datareader.moex\t\n",
    "    pandas_datareader.nasdaq_trader\t\n",
    "    pandas_datareader.naver\t\n",
    "    pandas_datareader.oecd\t\n",
    "    pandas_datareader.quandl\t\n",
    "    pandas_datareader.stooq\t\n",
    "    pandas_datareader.tiingo\t\n",
    "    pandas_datareader.tsp\t\n",
    "    pandas_datareader.wb\t\n",
    "    pandas_datareader.yahoo.actions\t\n",
    "    pandas_datareader.yahoo.components\t\n",
    "    pandas_datareader.yahoo.daily\t\n",
    "    pandas_datareader.yahoo.fx\t\n",
    "    pandas_datareader.yahoo.options\t\n",
    "    pandas_datareader.yahoo.quotes\n",
    "\n",
    "Note there are various sources for financial date (yahoo, fred, iex...). Some of these APIs have turned to paid versions, therefore make `pandas_datareader` not very useful. However, we keep it here for historical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's get some data. We first plot the Apple stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime(2015, 2, 15)\n",
    "end = datetime(2025, 2, 15)\n",
    "\n",
    "AAPL = yf.Ticker(\"AAPL\").history(start = start, end= end)\n",
    "\n",
    "print(type(AAPL))\n",
    "AAPL.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this data mean? \n",
    "* **high** is the highest price of the stock on that trading day, \n",
    "* **low** the lowest price of the stock on that trading day, \n",
    "* **Open** is the price of the stock at the beginning of the trading day (it need not be the closing price of the previous trading day)\n",
    "* **close** the price of the stock at closing time\n",
    "* **Volume** indicates how many stocks were traded \n",
    "* **Adj Closed** is the price of the stock after adjusting for corporate actions. While stock prices are considered to be set mostly by traders, *stock splits* (when the company makes each extant stock worth two and halves the price) and *dividends* (payout of company profits per share) also affect the price of a stock and should be accounted for.\n",
    "\n",
    "### Visualizing Stock Data\n",
    "\n",
    "Now that we have stock data we can visualize it using the `matplotlib` package, called using a convenience method, `plot()` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "AAPL[\"Close\"].plot(grid = True); # Plot the adjusted closing price of AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task 1.1: Plotting multiple stocks together\n",
    "\n",
    "For a variety of reasons, we may wish to plot multiple financial instruments together including:\n",
    "* we may want to compare stocks\n",
    "* compare them to the market or other securities such as [exchange-traded funds (ETFs)](https://en.wikipedia.org/wiki/Exchange-traded_fund).\n",
    "\n",
    "Below we want to plot the adjusted close value for several stocks together. Pick the stocks for META (ticker is `META`) and Google (ticker is `GOOG`) for the same timeframe as the Apple one and plot them together. Create a single dataframe with all 3 stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = ...\n",
    "GOOG = ...\n",
    "\n",
    "# Below I create a DataFrame consisting of the adjusted closing price of these stocks, first by making a list of these objects and using the join method\n",
    "close = pd.DataFrame(...)\n",
    "\n",
    "close.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#here we plot it\n",
    "close.plot(grid = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: What do you notice on these plot? How easy is to compare the stock prices of the different companies?\n",
    "\n",
    "*your comments go here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make comparable plots, we are going to plot the *stock returns since the beginning of the period of interest*:\n",
    "\n",
    "$$\n",
    "\\text{return}_{t,0} = \\frac{\\text{price}_t}{\\text{price}_0}\n",
    "$$\n",
    "\n",
    "This requires transforming the data. Do the transformation and save the data in a new data frame.\n",
    "\n",
    "*Hint: The transformation is easy if you apply a lambda function on the dataframe\n",
    "\n",
    "### Task 1.3: Create a dataframe to plot the stock return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a lambda function and then apply it on the dataframe\n",
    "stock_return = close.apply(...)\n",
    "stock_return.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we plot the result\n",
    "stock_return.plot(grid = True).axhline(y = 1, color = \"black\", lw = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task 1.4: Comment on the plot\n",
    "\n",
    "*Things to mention: Is it more useful? How are the stocks correlated? Could i see the same info on the previous plot?*\n",
    "\n",
    "*Your comments go here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: Compute other variations of the change of the series\n",
    "\n",
    "Alternatively, we could plot the change of each stock per day. One way to do so would be to use the *percentage increase of a stock*:\n",
    "$$\n",
    "\\text{increase}_t = \\frac{\\text{price}_{t} - \\text{price}_{t-1}}{\\text{price}_{t-1}}\n",
    "$$\n",
    "\n",
    "or the *log difference*.\n",
    "\n",
    "$$\n",
    "\\text{change}_t = \\log\\left( \\frac{\\text{price}_{t}}{\\text{price}_{t - 1}} \\right) = \\log(\\text{price}_{t}) - \\log(\\text{price}_{t - 1})\n",
    "$$\n",
    "\n",
    "Here, $\\log$ is the natural log. Log difference has a desirable property: the sum of the log differences can be interpreted as the total change (as a percentage) over the period summed. Log differences also more cleanly correspond to how stock prices are modeled in continuous time.\n",
    "\n",
    "Note also the transformations we discussed in class (box-cox transformation).\n",
    "\n",
    "In the code block below, obtain and plot the log differences of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_change = ...\n",
    "stock_change.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot stock change\n",
    "stock_change.plot(grid = True).axhline(y = 0, color = \"black\", lw = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Task 1.6: Would you go for the plot stock return or log difference? In which cases you would go for one or the other?\n",
    "\n",
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task 1.7: Comparing stocks to the overall market \n",
    "\n",
    "We often want to compare the performance of stocks to the performance of the overall market. \n",
    "[SPY](https://finance.yahoo.com/quote/SPY/) is the ticker symbol for the SPDR S&P 500 exchange-traded mutual fund (ETF), which is a fund that has roughly the stocks in the [S&P 500 stock index](https://finance.yahoo.com/quote/%5EGSPC?p=^GSPC). \n",
    "This serves as one measure for the overal market.\n",
    "\n",
    "Below we import the SPY series. \n",
    "\n",
    "You need to add it to the relevant dataframes and apply the same transformations. In the end plot the stock return and stock change plots again with the SPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = yf.Ticker(\"SPY\").history(start = start, end= end)\n",
    "SPY.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to close dataframe\n",
    "#add it to stock_return dataframe\n",
    "#add it to stock_change dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to close dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all stocks and SPY\n",
    "stock_return.plot(grid = True).axhline(y = 1, color = \"black\", lw = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to stock change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the stock change with SPY\n",
    "stock_change.plot(grid = True).axhline(y = 0, color = \"black\", lw = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 Moving Averages\n",
    "\n",
    "In class we discussed the idea of moving averages. Here we repeat it:\n",
    "\n",
    "For a time series $x_t$, the *$q$-day moving average at time $t$*, denoted $MA^q_t$, is the average of $x_t$ over the past $q$ days, \n",
    "$$\n",
    "MA^q_t = \\frac{1}{q} \\sum_{i = 0}^{q-1} x_{t - i}\n",
    "$$\n",
    "\n",
    "The [`rolling`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html) function in Pandas provides functionality for computing moving averages.\n",
    "\n",
    "### Task 1.8: Create a 20-day moving average for Apple stock data and plot it alongside the stock price. \n",
    "\n",
    "Use the `rolling` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute the 20d average\n",
    "AAPL[\"20d\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how late the rolling average begins. It cannot be computed until twenty days have passed. Note that this becomes more severe for slower moving averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot both the actual price and the 20-d average.\n",
    "AAPL[[\"Close\", \"20d\"]].tail(300).plot(grid = True); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the moving averages \"smooths\" the time series. This can sometimes make it easier to identify trends. The larger $q$, the less responsive a moving average is to fast fluctuations in the series $x_t$. \n",
    "So, if these fast fluctuations are considered \"noise\", a moving average will identify the \"signal\". \n",
    "* *Fast moving averages* have smaller $q$ and more closely follow the time series. \n",
    "* *Slow moving averages* have larger $q$ and respond less to the fluctuations of the stock.\n",
    "\n",
    "### Task 1.9: Let's compare the 20-day, 50-day, and 200-day moving averages\n",
    "\n",
    "Compute them below and then plot them together with the stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AAPL[\"50d\"] = ...\n",
    "AAPL[\"200d\"] = ...\n",
    "\n",
    "#plotting\n",
    "AAPL[[\"Close\", \"20d\", \"50d\", \"200d\"]].tail(500).plot(grid = True); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trading strategies and backtesting \n",
    "\n",
    "\n",
    "**Trading** is the practice of buying and selling financial assets for the purpose of making a profit. Traders develop **trading strategies** that a computer can use to make trades. Sometimes, these can be very complicated (esp. when they employ exogenous factors and data like [Twitter](https://www.theatlantic.com/technology/archive/2011/03/does-anne-hathaway-news-drive-berkshire-hathaways-stock/72661/), but other times traders make decisions based on finding patterns or trends in charts. \n",
    "\n",
    "One example is called the [moving average crossover strategy](http://www.investopedia.com/university/movingaverage/movingaverages4.asp). \n",
    "This strategy is based on two moving averages, a \"fast\" one and a \"slow\" one. The strategy is:\n",
    "\n",
    "* Trade the asset when the fast moving average crosses over the slow moving average.\n",
    "* Exit the trade when the fast moving average crosses over the slow moving average again.\n",
    "\n",
    "A trade will be prompted when the fast moving average crosses from below to above the slow moving average, and the trade will be exited when the fast moving average crosses below the slow moving average later.\n",
    "\n",
    "This is the outline of a complete strategy and we already have the tools to get a computer to automatically implement the strategy.\n",
    "\n",
    "But before we decide if we want to use it, we should first evaluate the quality of the strategy. The usual means for doing this is called **backtesting**, which is looking at how profitable the strategy is on historical data. \n",
    "\n",
    "You could now write python code that could implement and backtest a trading strategy. There are also lots of python packages for this:  \n",
    "* [**pyfolio**](https://quantopian.github.io/pyfolio/) (for analytics)\n",
    "* [**zipline**](http://www.zipline.io/beginner-tutorial.html) (for backtesting and algorithmic trading), and \n",
    "* [**backtrader**](https://www.backtrader.com/) (also for backtesting and trading). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.10: A simple trading strategy\n",
    "\n",
    "Use the plot above (assuming that our 20-day moving average is the slow moving one and the 50-day moving average is the fast moving one) and based on the strategy we described above, determine when you should have bought (and also sold) Apple stock?\n",
    "\n",
    "*your comments go here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: A forecasting problem\n",
    "\n",
    "We will now apply some forecasting algorithms on a real dataset, namely the Berkeley Earth Surface Temperature Study. Dataset can be fount on [Berkeley Earth data](http://berkeleyearth.org/data/) page and combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries (again?)\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/GlobalLandTemperaturesByMajorCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first few records\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a concise summary of the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing, Advanced Visualisation and Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's isolate Toronto and consider the data of this city to be our dataset. The target is the `AverageTemperature` column, that is the Average Temperature for that specific month. We have data from 1743 to 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_data = data[data['City'] == 'Toronto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 The `AvergateTemperature` has some missing data. Determine how many they are and find a (suitable) technique to fill it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "toronto_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dt` column identifies the year and the month. It is better to convert this column into a datetime object and to explicitly identify the year and the month in two different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_data = toronto_data.reset_index()\n",
    "toronto_data = toronto_data.drop(columns = ['index'])\n",
    "toronto_data.dt = pd.to_datetime(toronto_data.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = []\n",
    "MONTH = []\n",
    "DAY = []\n",
    "WEEKDAY = []\n",
    "for i in range(len(toronto_data)):\n",
    "    WEEKDAY.append(toronto_data.dt[i].weekday())\n",
    "    DAY.append(toronto_data.dt[i].day)\n",
    "    MONTH.append(toronto_data.dt[i].month)\n",
    "    YEAR.append(toronto_data.dt[i].year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_data['Year'] = YEAR\n",
    "toronto_data['Month'] = MONTH\n",
    "toronto_data['Day'] = DAY \n",
    "toronto_data['Weekday'] = WEEKDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_year_index = []\n",
    "change_year = []\n",
    "year_list = toronto_data['Year'].tolist()\n",
    "for year in range(0, len(year_list) - 1):\n",
    "    if year_list[year] != year_list[year + 1]:\n",
    "        change_year.append(year_list[year + 1])\n",
    "        change_year_index.append(year + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_data.loc[change_year_index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can describe three functions:\n",
    "1. `get_timeseries(start_year,end_year)`:  extracts the portion of the dataset between the two years \n",
    "1. `plot_timeseries(start_year,end_year)`:  plots the timeseries extracted in get_timeseries in a readable way\n",
    "1. `plot_from_data(data, time, display_options)`:  plots the data (AverageTemperature) wrt the time (dt) in a readable way. The display options permit to display the ticks, change the colors, set the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeseries(start_year,end_year):\n",
    "    last_year_data = toronto_data[(toronto_data.Year >= start_year) & (toronto_data.Year <= end_year)].reset_index().drop(columns = ['index'])\n",
    "    return last_year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(start_year,end_year):\n",
    "    last_year_data = get_timeseries(start_year,end_year)\n",
    "    P = np.linspace(0, len(last_year_data) - 1, 5).astype(int)\n",
    "    plt.plot(last_year_data.AverageTemperature, marker = '.', color = '#003366')\n",
    "    plt.xticks(np.arange(0, len(last_year_data), 1)[P], last_year_data.dt.loc[P])\n",
    "    plt.xlabel('Date (Y/M/D)')\n",
    "    plt.ylabel('Average Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified\n",
    "def plot_from_data(df, time_col, c='#003366', with_ticks=True, label=None):\n",
    "    # Plot the DataFrame directly using the pandas plot method\n",
    "    # This will automatically use the time column for the x-axis and the data column for the y-axis\n",
    "    ax = df.plot(x=time_col, y=df.values, marker='.', color=c, label=label, figsize=(12, 7))\n",
    "    #   ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) \n",
    "    # Handle x-ticks to avoid overlap\n",
    "    if with_ticks:\n",
    "        ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "    ax.set_xlabel('Date (Y/M/D)')\n",
    "    ax.set_ylabel('Average Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example use of plotting data for different decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "# plt.title(figure_title, )\n",
    "plt.suptitle('Plotting Four Decades', fontsize = 30, color = '#003366', y = 1.03)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Starting year: 1800, Ending Year: 1810', fontsize = 15)\n",
    "plot_timeseries(1800, 1810)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Starting year: 1900, Ending Year: 1910', fontsize = 15)\n",
    "plot_timeseries(1900, 1910)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Starting year: 1950, Ending Year: 1960', fontsize = 15)\n",
    "plot_timeseries(1950, 1960)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Starting year: 2000, Ending Year: 2010', fontsize = 15)\n",
    "plot_timeseries(2000, 2010)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use ARIMA models, we should be considering stationary time series. In order to check if the timeseries we are considering is stationary, we can check the correlation and autocorrelation plots.\n",
    "\n",
    "### Task 2.2: Plot the ACF/PACF and make observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot acf and pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your comments here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Run the ADF statistic test on the whole series and then on one decade. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(toronto_data.AverageTemperature)\n",
    "print('ADF Statistic on the entire dataset: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(toronto_data.AverageTemperature[0:120])\n",
    "print('ADF Statistic on the first decade: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Take differences and try to stationarize the series\n",
    "\n",
    "If we take differencing does the timeseries become stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#your comments as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the 1992–2013 decade and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "plt.title('The dataset used for prediction', fontsize = 20, color = '#003366')\n",
    "plot_timeseries(1992, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data to training and testing and plot it.\n",
    "\n",
    "Remember that it's timeseries, therefore the last past will always be used as validation and the first part of the series as training. Any deviation might lead to data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the train/test split.\n",
    "temp = get_timeseries(1992, 2013)\n",
    "N = len(temp.AverageTemperature)\n",
    "split = 0.95\n",
    "training_size = round(split * N)\n",
    "print(training_size)\n",
    "test_size = round((1 - split) * N)\n",
    "print(test_size)\n",
    "series = temp.AverageTemperature[ : training_size]\n",
    "date = temp.dt[:training_size]\n",
    "series_df = pd.Series(series.values,index=date)\n",
    "\n",
    "# Test data (last 5%)\n",
    "test_series = temp.AverageTemperature[training_size:]\n",
    "test_date = pd.to_datetime(temp.dt[training_size:])\n",
    "test_series_df = pd.Series(test_series.values, index=test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the split\n",
    "plt.figure(figsize = (12, 7))\n",
    "plot_from_data(series, date, label = 'Training Set')\n",
    "plot_from_data(test_series, test_date, 'green', with_ticks = True, label = 'Test Set')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = temp.AverageTemperature[len(date)-1 : len(temp)]\n",
    "test_date = temp.dt[len(date)-1 : len(temp)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply ARIMA models and more specifically we are going to use a function to determine what is the best ARIMA Model based on a range of values for p and q. Study the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ARIMA(order_list, exog):\n",
    "    \"\"\"\n",
    "    Optimize ARIMA model by testing different (p, d, q) orders and returning a dataframe\n",
    "    with the corresponding Akaike Information Criterion (AIC) scores.\n",
    "\n",
    "    Parameters:\n",
    "        order_list (list of tuples): List of (p, d, q) tuples to evaluate.\n",
    "        exog (array-like): Exogenous variable for the ARIMA model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing (p, d, q) orders and their respective AIC scores,\n",
    "                      sorted in ascending order (lower AIC is better).\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []  # Store model results as (order, AIC) pairs\n",
    "    \n",
    "    for order in tqdm(order_list):  # Iterate through each (p, d, q) combination\n",
    "        try:\n",
    "            # Fit the ARIMA model with the given order\n",
    "            model = SARIMAX(exog, order=order).fit(disp=-1)\n",
    "            \n",
    "            # Get the AIC value for model evaluation\n",
    "            aic = model.aic  \n",
    "            \n",
    "            # Store results as a list of tuples\n",
    "            results.append([order, aic])\n",
    "            \n",
    "            # Convert results into a DataFrame\n",
    "            result_df = pd.DataFrame(results, columns=['(p, d, q)', 'AIC'])\n",
    "            \n",
    "            # Sort results by AIC in ascending order (lower is better)\n",
    "            result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle cases where a model fails to fit\n",
    "            print(f\"Skipping order {order} due to error: {e}\")\n",
    "        \n",
    "        continue  # Continue iterating even if an exception occurs\n",
    "    \n",
    "    return result_df  # Return the sorted DataFrame of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take the zero-differentiated ARIMA models and evaluate them using the AIC.\n",
    "\n",
    "### Task 2.5 Pick some reasonable upper bounds for p and q and run the code below. Print the results on comment on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp = ... #determine the the maximum order p\n",
    "maxq = ... #determine the the maximum order q\n",
    "\n",
    "ps = range(0, maxp, 1)\n",
    "d = 0\n",
    "qs = range(0, maxq, 1)\n",
    "\n",
    "# Create a list with all possible combination of parameters\n",
    "parameters = product(ps, qs)\n",
    "parameters_list = list(parameters)\n",
    "\n",
    "order_list = []\n",
    "\n",
    "for each in parameters_list:\n",
    "    each = list(each)\n",
    "    each.insert(1, d)\n",
    "    each = tuple(each)\n",
    "    order_list.append(each)\n",
    "    \n",
    "result_d_0 = optimize_ARIMA(order_list, exog = series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_d_0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment on the results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first-differentiated models are considered below.\n",
    "\n",
    "### Task 2.6 Pick some reasonable upper bounds for p and q and run the code below. Print the results on comment on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp=...\n",
    "maxq=...\n",
    "\n",
    "ps = range(0, maxp, 1)\n",
    "d = 1\n",
    "qs = range(0, maxq, 1)\n",
    "\n",
    "# create a list with all possible combination of parameters\n",
    "parameters = product(ps, qs)\n",
    "parameters_list = list(parameters)\n",
    "\n",
    "order_list = []\n",
    "\n",
    "for each in parameters_list:\n",
    "    each = list(each)\n",
    "    each.insert(1, d)\n",
    "    each = tuple(each)\n",
    "    order_list.append(each)\n",
    "    \n",
    "result_d_1 = optimize_ARIMA(order_list, exog = series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment on the results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7 Pick a model based on how good it performs (take into account small variations in the performance) and plot the residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_params= ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SARIMAX(series, order = best_model_params).fit()\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit diagnostics and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_diagnostics(figsize = (15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecasting\n",
    "\n",
    "In the last part we are going to use our best model for forecasting and evaluate it's performance. Most of the code is given but you just need to interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results of the forecasting operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_l= test_size - 1\n",
    "forecast = best_model.get_prediction(start = training_size, end = training_size + fore_l)\n",
    "forec = forecast.predicted_mean\n",
    "ci = forecast.conf_int(alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = temp.AverageTemperature[len(date)-1 : len(temp)]\n",
    "error_test = toronto_data.loc[test_date[1:].index.tolist()].AverageTemperatureUncertainty\n",
    "index_test = test_date[1:].index.tolist()\n",
    "test_set = test_series[1:]\n",
    "lower_test = test_set - error_test\n",
    "upper_test = test_set + error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = toronto_data.AverageTemperature.index[0 : training_size]\n",
    "x1 = toronto_data.AverageTemperature.index[training_size : training_size + fore_l + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8), dpi=300)\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(x0, toronto_data.AverageTemperature[0:training_size], 'k', label='Training Set')\n",
    "\n",
    "# Plot actual test data\n",
    "plt.plot(toronto_data.AverageTemperature[training_size:training_size + fore_l], '.k', label='Actual Test Data')\n",
    "\n",
    "# Plot forecast directly from forec\n",
    "ax.plot(x1, forec, color='Darkorange', label='Forecast (best_model_0)', marker='.')\n",
    "ax.fill_between(x1, ci['lower AverageTemperature'], ci['upper AverageTemperature'], \n",
    "                alpha=0.2, label='Confidence Interval (95%)', color='grey')\n",
    "\n",
    "# Plot lower and upper bounds for the test set\n",
    "ax.fill_between(x1, lower_test, upper_test, alpha=0.2, label='Test Set Bounds', color='lightgreen')\n",
    "\n",
    "# Add legend, labels, and title\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(80,)\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Forecast vs Actual Temperature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's consider the specific predicted zone with the correspondent Uncertainty (the one given by the dataset) and the confidence interval (given by the algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 15))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.fill_between(x1, lower_test, upper_test, alpha = 0.2, label = 'Test set error range', color = 'navy')\n",
    "plt.plot(test_set, marker = '.', label = 'Actual', color = 'navy')\n",
    "plt.plot(forec, marker = 'd', label = 'Forecast', color = 'green')\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature')\n",
    "#plt.fill_between(x1, s_ci['lower AverageTemperature'], s_ci['upper AverageTemperature'],alpha=0.3, label = 'Confidence inerval (95%)',color='firebrick')\n",
    "plt.legend()\n",
    "#plt.subplot(2, 1, 2)\n",
    "#plt.fill_between(x1, lower_test, upper_test,alpha=0.2, label = 'Test set error range',color='navy')\n",
    "#plt.plot(test_set, marker = '.', label = 'Actual', color = 'navy')\n",
    "#plt.plot(s_forec, marker = 'd', label = 'Forecast', color = 'firebrick')\n",
    "#plt.fill_between(x1, ci['lower AverageTemperature'], ci['upper AverageTemperature'], alpha = 0.3, label = 'Confidence inerval (95%)', color = 'firebrick')\n",
    "plt.legend()\n",
    "plt.xlabel('Index Datapoint')\n",
    "plt.ylabel('Temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.8 Comment on the forecasting result. Is it good or bad? Can you only judge from the plot?\n",
    "\n",
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Task 2.9 Finally, compute the Mean Absolute Error (MAE) and the Mean Squared Error (MSE) for the predictions on the test set. Comment on what do these values mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#forec should hold the prediction\n",
    "#test_series should hold the real value\n",
    "\n",
    "#MAE\n",
    "...\n",
    "\n",
    "#MSE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
